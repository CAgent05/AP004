# Machine Translation Inference Guide

This document provides instructions for using the `infer.py` script to perform Chinese-to-English translation using pre-trained Transformer or RNN (GRU) models.

## Prerequisites

Before running inference, ensure you have the following:

1. **Pre-trained model checkpoint** (`.pt` or `.pth` file)
2. **Training data directory** containing the original dataset (required for vocabulary reconstruction)
3. **Cache directory** where vocabulary files are stored
4. **Required dependencies** installed (PyTorch, HanLP, etc.)

## Command Line Arguments

| Argument | Type | Required | Default | Description |
|----------|------|----------|---------|-------------|
| `--arch` | str | Yes | - | Model architecture: `rnn` or `transformer` |
| `--model_path` | str | Yes | - | Path to the trained model checkpoint |
| `--data_path` | str | No | `./data/` | Directory containing training data |
| `--cache_dir` | str | No | `./cache/` | Directory for cached vocabulary files |
| `--frequency` | int | No | `5` | Minimum word frequency for vocabulary |
| `--max_len` | int | No | `256` | Maximum decoding sequence length |
| `--decode_method` | str | No | `beam` | Decoding strategy: `greedy` or `beam` |
| `--beam_size` | int | No | `5` | Beam size for beam search decoding |

## Usage Examples

### Running with Transformer Model

~~~bash
python infer.py \
    --arch transformer \
    --model_path ./best_trans.pt \
~~~

### Running with RNN (GRU) Model

~~~bash
python infer.py \
    --arch rnn \
    --model_path ./best_rnn.pt \
~~~

## Interactive Commands

Once the inference system is running, you can use the following commands within the interactive shell:

| Command | Action |
|---------|--------|
| `q` or `exit` | Quit the program |
| `greedy` | Switch to greedy search decoding |
| `beam N` | Switch to beam search with beam size N (e.g., `beam 5`) |

## Example Session

~~~text
--- Initializing [TRANSFORMER] Translation System ---
Loading weights: ./checkpoints/transformer_best.pt

============================================================
 ðŸš€ Unified Translation Platform | Current Architecture: TRANSFORMER
 Default Mode: beam (Beam Size: 5)
============================================================
 Commands: 'q' to quit | 'greedy' for greedy search | 'beam N' for beam search

[ä¸­æ–‡] >>> ä»Šå¤©å¤©æ°”å¾ˆå¥½
[è‹±æ–‡] >>> The weather is very nice today.

[ä¸­æ–‡] >>> greedy
Switched to Greedy Search

[ä¸­æ–‡] >>> æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯
[è‹±æ–‡] >>> Machine learning is a branch of artificial intelligence.

[ä¸­æ–‡] >>> beam 10
Switched to Beam Search (Size=10)

[ä¸­æ–‡] >>> q
~~~

## Model Configuration

The script uses the following default model configurations:

**Transformer Model:**
- Embedding dimension: 256
- Number of attention heads: 8
- Number of layers: 3
- Feed-forward dimension: 2048
- Positional encoding: Absolute
- Normalization: RMSNorm

**RNN Model:**
- Embedding dimension: 300
- Hidden dimension: 512
- RNN type: GRU
- Attention type: Multiplicative

## File Structure

~~~text
project/
â”œâ”€â”€ infer.py           # Inference script
â”œâ”€â”€ transformer.py     # Transformer model definition
â”œâ”€â”€ GRU.py             # RNN/GRU model definition
â”œâ”€â”€ utils.py           # Data loading and tokenization utilities
â”œâ”€â”€ data/              # Training data directory
â”œâ”€â”€ cache/             # Vocabulary cache directory
â””â”€â”€ checkpoints/       # Model checkpoint directory
~~~